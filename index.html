import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin, urlparse
import time

# Function to scrape the website
def scrape_accordtraining():
    base_url = "https://accordtraining.com"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        # Get the main page
        response = requests.get(base_url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract key information
        data = {
            'title': '',
            'description': '',
            'courses': [],
            'contact': {},
            'about': '',
            'features': [],
            'links': []
        }
        
        # Get title
        title_tag = soup.find('title')
        if title_tag:
            data['title'] = title_tag.get_text().strip()
        
        # Get meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            data['description'] = meta_desc.get('content', '').strip()
        
        # Get headings and main content
        headings = soup.find_all(['h1', 'h2', 'h3'])
        main_content = []
        for heading in headings[:10]:  # Limit to first 10 headings
            main_content.append({
                'tag': heading.name,
                'text': heading.get_text().strip()
            })
        
        # Get all text paragraphs
        paragraphs = soup.find_all('p')
        text_content = [p.get_text().strip() for p in paragraphs[:15] if p.get_text().strip()]
        
        # Get links
        links = soup.find_all('a', href=True)
        for link in links[:20]:  # Limit to first 20 links
            href = link.get('href')
            text = link.get_text().strip()
            if text and href:
                data['links'].append({
                    'text': text,
                    'url': urljoin(base_url, href)
                })
        
        # Get contact information
        email_pattern = soup.find_all(text=lambda text: text and '@' in text)
        phone_pattern = soup.find_all(text=lambda text: text and any(char.isdigit() for char in str(text)))
        
        print("Scraped data successfully!")
        return {
            'data': data,
            'main_content': main_content,
            'text_content': text_content,
            'base_url': base_url
        }
        
    except Exception as e:
        print(f"Error scraping website: {e}")
        return None

# Scrape the website
scraped_data = scrape_accordtraining()

if scraped_data:
    print(f"Title: {scraped_data['data']['title']}")
    print(f"Description: {scraped_data['data']['description']}")
    print(f"Found {len(scraped_data['main_content'])} headings")
    print(f"Found {len(scraped_data['text_content'])} paragraphs")
    print(f"Found {len(scraped_data['data']['links'])} links")
else:
    print("Using fallback data...")
    scraped_data = {
        'data': {
            'title': 'Accord Training - Professional IT Training',
            'description': 'Leading IT training provider',
            'links': []
        },
        'main_content': [
            {'tag': 'h1', 'text': 'Accord Training'},
            {'tag': 'h2', 'text': 'Professional IT Training Courses'}
        ],
        'text_content': [
            'Welcome to Accord Training',
            'We provide professional IT training and certification courses.'
        ],
        'base_url': 'https://accordtraining.com'
    }
